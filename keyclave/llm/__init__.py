"""
LLM module - Local language model inference (optional).

Provides offline LLM capabilities via llama.cpp or Ollama for key detection assistance.
"""
